---
---

@string{aps = {American Physical Society,}}

@article{hodlr2d,
  abbr={SIAM},
  selected={true},
  bibtex_show={true},
  author   = {Kandappan, V A and Gujjula, Vaishnavi and Ambikasaran, Sivaram},
  title    = {HODLR2D: A New Class of Hierarchical Matrices},
  journal  = {SIAM Journal on Scientific Computing},
  volume   = {45},
  number   = {5},
  pages    = {A2382-A2408},
  year     = {2023},
  doi      = {10.1137/22M1491253},
  eprint   = {{https://doi.org/10.1137/22M1491253}},
  abstract = { Abstract. This article introduces HODLR2D, a new hierarchical low-rank representation for a class of dense matrices arising out of \(N\) -body problems in two dimensions. Using this new hierarchical framework, we propose a new fast matrix-vector product that scales almost linearly. We apply this fast matrix-vector product to accelerate the iterative solution of large dense linear systems arising out of radial basis function interpolation and discretized integral equation. The space and computational complexity of HODLR2D matrix-vector products scale as \(\mathcal{O}(pN \log (N))\) , where \(p\) is the maximum rank of the compressed matrix subblocks. For the logarithmic kernel function, we prove that \(p \in \mathcal{O} \left (\log \left (N\right ) \log \left (\log \left (N\right ) \right ) \right )\) . We also numerically observe a similar scaling for other kernel functions in 2D. We thereby demonstrate that the storage and computational complexity of HODLR2D matrix-vector products remain tractable for large \(N\) . Additionally, we also study the parallel scalability of HODLR2D as part of this article. },
  preview = {hodlr2d.gif}
}

@inproceedings{mlrnn,
  abbr={SPRINGER},
  bibtex_show={true},
  author    = {Kandappan, V A and Rekha, A. G.},
  title     = {Machine Learning in Finance: Towards Online Prediction of Loan Defaults Using Sequential Data with LSTMs},
  booktitle = {Soft Computing: Theories and Applications},
  year      = {2021},
  publisher = {Springer Singapore},
  address   = {Singapore},
  pages     = {53--62},
  abstract  = {In recent times, machine learning is finding many important use cases in the financial services industry. Financial datasets usually pose significant statistical challenges, and hence, traditional econometric methods fail in many practical applications. Though several approaches are suggested and being used, to predict whether a borrower would default on the loan availed, it is still an open challenge. In this work, two approaches are proposed based on LSTM (Long Short Term Memory) along with a hybrid neural network architecture to understand the context between financial transactions and loan defaults. The novelty of the proposed methods is in how they handle structured data and the associated temporal data. Further, the performance of the proposed approaches using debit card transactions and loan application information of customers to predict default is demonstrated. The experiments provided promising accuracies. Bidirectional LSTMs with a hybrid architecture achieve an accuracy of 94{\%}. Hybrid neural network architectures used in this work provide an appropriate direction for making an early warning system through online loan default prediction.Kandappan, V. A.Rekha, A. G.},
  isbn      = {978-981-16-1696-9}
}

@article{pmu,
  abbr={IJAER},
  bibtex_show={true},
  author  = {Venugopal, Gomathi and Kandappan, V A},
  year    = {2015},
  month   = {12},
  title   = {Effect of time sychronization error in phasor measurement unit},
  volume  = {10},
  journal = {International journal of applied engineering research}
}

@misc{kandappan2023hodlr3d,
  abbr={arXiv},
  bibtex_show={true},
  title         = {HODLR3D: Hierarchical matrices for $N$-body problems in three dimensions},
  author        = {V A Kandappan and Vaishnavi Gujjula and Sivaram Ambikasaran},
  year          = {2023},
  eprint        = {2307.16303},
  archiveprefix = {arXiv},
  primaryclass  = {math.NA}
}

@article{khan2023hodlrdd,
  abbr={JCP},
  bibtex_show={true},
title = {HODLRdD: A new black-box fast algorithm for N-body problems in d-dimensions with guaranteed error bounds: Applications to integral equations and support vector machines},
journal = {Journal of Computational Physics},
volume = {501},
pages = {112786},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2024.112786},
url = {https://www.sciencedirect.com/science/article/pii/S0021999124000354},
author = {Ritesh Khan and V A Kandappan and Sivaram Ambikasaran},
abstract = {We study rank of sub-matrices arising out of kernel functions, F(x,y):Rd×Rd↦R, where x,y∈Rd and F(x,y) is smooth everywhere except along the line y=x. Such kernel functions are frequently encountered in a wide range of areas. To our knowledge, this is the first work to formally study rank growth of matrices arising out of a wide range of kernel functions in any dimension. In this article, we prove two new theorems bounding rank of different sub-matrices arising from these kernel functions. Bounds like these are often useful for analyzing the complexity of various hierarchical matrix algorithms. We also plot the numerical rank growth of different sub-matrices arising out of various kernel functions in 1D, 2D, 3D and 4D, which agrees with the proposed theorems. Another significant contribution of this article is that using the obtained rank bounds, we also propose a way to extend the notion of weak-admissibility for hierarchical matrices in higher dimensions. Based on this proposed weak-admissibility condition, we develop a black-box (kernel-independent) fast algorithm for N-body problems, hierarchically off-diagonal low-rank matrix in d dimensions (HODLRdD), which can perform matrix-vector products with O(pNlog⁡(N)) complexity in any dimension d. Our theorems guarantee that p does not grow with any positive power of N, which implies our HODLRdD algorithm scales almost linearly.1 The C++ implementation with OpenMP parallelization of the HODLRdD is available at https://github.com/SAFRAN-LAB/HODLRdD. We also discuss the scalability of the HODLRdD algorithm and showcase the applicability by solving an integral equation in 4D and accelerating the training phase of the support vector machines (SVM) for the data sets with four and five features.1The proposed algorithm scales almost linearly, i.e., O(Nlogα⁡(N)), α>0.}
}
